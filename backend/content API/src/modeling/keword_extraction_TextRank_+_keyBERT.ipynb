{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HetokZ84DT0e"
      },
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tonmKxNBC6s7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import urllib.request\n",
        "from collections import Counter\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ixX19lmm8zs"
      },
      "outputs": [],
      "source": [
        "# TextRank\n",
        "from gensim.summarization.summarizer import summarize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkce62xgzA5c"
      },
      "outputs": [],
      "source": [
        "# kiwi: Tokenizer\n",
        "from kiwipiepy import Kiwi\n",
        "# keyBert\n",
        "from keybert import KeyBERT\n",
        "from transformers import BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tT4-deliwcQM"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.clear()\n",
        "sys.path.append('/'.join(os.getcwd().split(\"\\\\\")[:-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0LF5cA5zCmb"
      },
      "source": [
        "## data 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogEaZdn7KKkM"
      },
      "source": [
        "### Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWo2YUCyKScN"
      },
      "outputs": [],
      "source": [
        "kiwi = Kiwi()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZPxj9iuKSfX"
      },
      "outputs": [],
      "source": [
        "# 명사 추출 함수\n",
        "def noun_extractor(text):\n",
        "    results = []\n",
        "    result = kiwi.analyze(text)\n",
        "    # print(result)\n",
        "    for token, pos, _, _ in result[0][0]:\n",
        "        if len(token) != 1 and pos.startswith('N') or pos.startswith('SL'):\n",
        "            results.append(token)\n",
        "    return results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 불용어 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stopwords = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from crawling import config\n",
        "#  from src.crawling.config import connect, close\n",
        "\n",
        "conn = config.connect()\n",
        "curs = conn.cursor()\n",
        "\n",
        "channel_sql = \"\"\"select name from channel group by name\"\"\"\n",
        "curs.execute(channel_sql)\n",
        "channel_info = curs.fetchall()\n",
        "\n",
        "channels = []\n",
        "for channel in channel_info:\n",
        "    channels.extend(channel)\n",
        "print(channels)\n",
        "\n",
        "config.close()\n",
        "\n",
        "stopwords.extend(channels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLHwJBTIyx9_"
      },
      "source": [
        "## keyword 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-uzs8GQyslO"
      },
      "outputs": [],
      "source": [
        "def keyword_extractor(bert:str, documents:List[str]) -> (List[Tuple[str, float]] | List[List[Tuple[str, float]]]):\n",
        "  model = BertModel.from_pretrained(bert)\n",
        "  model = model.cuda()\n",
        "  kw_model = KeyBERT(model)\n",
        "  keywords = kw_model.extract_keywords(documents, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=20)\n",
        "  return keywords\n",
        "# model_100langs = BertModel.from_pretrained('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
        "# kw_model_100langs = KeyBERT(model_100langs)\n",
        "# keywords_100langs = kw_model_100langs.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# gpu 사용법\n",
        "# https://github.com/MaartenGr/KeyBERT/issues/108\n",
        "# model = SentenceTransformer(\n",
        "#     \"<hf_model_name>\",\n",
        "#     device=\"cuda:0\"\n",
        "# )\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "model = model.cuda()\n",
        "print(\"DEVICE\", model.device)\n",
        "# hf_model = KeyBERT(model)\n",
        "\n",
        "# https://www.kaggle.com/code/accountstatus/shopee-competition-using-bert-model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXJC5qTf6j3n"
      },
      "outputs": [],
      "source": [
        "# print(os.getcwd())\n",
        "# base_path = \"/content/drive/MyDrive/쳇, 6pt: 비스킷(biskuit)/dataset/content/\"\n",
        "base_path = '../../../data/content/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64ezQHBbiJ9K"
      },
      "outputs": [],
      "source": [
        "def extract_keyword(base_path:str):\n",
        "  content_dir_list = os.scandir(base_path)\n",
        "  documents = []\n",
        "  for content_dir in content_dir_list:\n",
        "      if not content_dir.is_dir(): continue\n",
        "      content_list = os.scandir(content_dir)\n",
        "      for content in content_list:\n",
        "          if not content.is_file(): continue\n",
        "          file = open(content.path, \"r\")\n",
        "          if len(text.split('\\n')) > 50:\n",
        "            summary = summarize(text)\n",
        "          else:\n",
        "            summary = text\n",
        "          # nouns = noun_extractor(text)\n",
        "          nouns = noun_extractor(summary)\n",
        "          text = ' '.join(nouns)\n",
        "          documents.append(text)\n",
        "  # print(text)\n",
        "  keyword_extractor('skt/kobert-base-v1', documents)\n",
        "  # keyword_extractor('paraphrase-multilingual-MiniLM-L12-v2', documents)   # multilingual!\n",
        "  # keyword_extractor('paraphrase-multilingual-mpnet-base-v2', documents)   # 시간 더 오래걸리고 결과 더 조음!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "faster_keywords = extract_keyword(base_path)\n",
        "faster_counter = Counter(faster_keywords)\n",
        "print(faster_counter.most_common(100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgA6BUyFa3qR"
      },
      "outputs": [],
      "source": [
        "# file = open(data_path + \"2022-09-22_29CM_예약하기 서비스 개발기 (1)\" + \".txt\", \"r\")\n",
        "content_dir_list = os.scandir(base_path)\n",
        "keywords = [] \n",
        "for content_dir in tqdm(content_dir_list):\n",
        "  if not content_dir.is_dir(): continue\n",
        "  content_list = os.scandir(content_dir)\n",
        "  for content in content_list:\n",
        "    if not content.is_file(): continue\n",
        "    file = open(content.path, \"r\")\n",
        "    keywords.append(extract_keyword(file))\n",
        "counter = Counter(keywords)\n",
        "print(counter.most_common(100))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
