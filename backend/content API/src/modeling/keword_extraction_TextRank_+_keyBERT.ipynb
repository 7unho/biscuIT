{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HetokZ84DT0e"
      },
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "tonmKxNBC6s7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "%matplotlib inline\n",
        "import sys\n",
        "from collections import Counter\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "2ixX19lmm8zs"
      },
      "outputs": [],
      "source": [
        "# TextRank\n",
        "from gensim.summarization.summarizer import summarize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "hkce62xgzA5c"
      },
      "outputs": [],
      "source": [
        "# kiwi: Tokenizer\n",
        "from kiwipiepy import Kiwi\n",
        "# keyBert\n",
        "from keybert import KeyBERT\n",
        "from transformers import BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "tT4-deliwcQM"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "module_path = '/'.join(os.getcwd().split(\"\\\\\")[:-1])\n",
        "sys.path.append(module_path)\n",
        "sys.path.append(module_path + '/crawling')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0LF5cA5zCmb"
      },
      "source": [
        "## data 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogEaZdn7KKkM"
      },
      "source": [
        "### Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "mWo2YUCyKScN"
      },
      "outputs": [],
      "source": [
        "kiwi = Kiwi()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "1ZPxj9iuKSfX"
      },
      "outputs": [],
      "source": [
        "# 명사 추출 함수\n",
        "def noun_extractor(text):\n",
        "    results = []\n",
        "    result = kiwi.analyze(text)\n",
        "    # print(result)\n",
        "    for token, pos, _, _ in result[0][0]:\n",
        "        if len(token) != 1 and pos.startswith('N') or pos.startswith('SL'):\n",
        "            results.append(token)\n",
        "    return results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 불용어 제거"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### stopword: list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "stopwords = [\n",
        "    # channel\n",
        "    \"11번가\",\n",
        "    \"11st-tech\",\n",
        "    \"29CM\",\n",
        "    \"29cm\",\n",
        "    \"AWS\",\n",
        "    \"amazon\",\n",
        "    \"class101\",\n",
        "    \"GS리테일\",\n",
        "    \"gsretail\",\n",
        "    \"NHN\",\n",
        "    \"nhncloud\",\n",
        "    \"ToastUI\",\n",
        "    \"네이버D2\",\n",
        "    \"d2\",\n",
        "    \"naver\",\n",
        "    \"네이버클라우드\",\n",
        "    \"naver-cloud-platform\",\n",
        "    \"네이버플레이스\",\n",
        "    \"naver-place-dev\",\n",
        "    \"넷마블\",\n",
        "    \"netmarble\",\n",
        "    \"다나와\",\n",
        "    \"danawalab\",\n",
        "    \"당근마켓\",\n",
        "    \"daangn\",\n",
        "    \"데보션\",\n",
        "    \"devocean\",\n",
        "    \"sk\",\n",
        "    \"데브시스터즈\",\n",
        "    \"devsisters\",\n",
        "    \"드라마앤컴퍼니\",\n",
        "    \"dramancompany\",\n",
        "    \"라인\",\n",
        "    \"linecorp\",\n",
        "    \"레모네이드\",\n",
        "    \"lemonade-engineering\",\n",
        "    \"롯데on\",\n",
        "    \"lotteon\",\n",
        "    \"루닛\",\n",
        "    \"lunit\",\n",
        "    \"마이리얼트립\",\n",
        "    \"myrealtrip-product\",\n",
        "    \"메가존클라우드\",\n",
        "    \"ctc-mzc\",\n",
        "    \"무신사\",\n",
        "    \"musinsa\",\n",
        "    \"버즈빌\",\n",
        "    \"buzzvil\",\n",
        "    \"브랜디\",\n",
        "    \"brandi\",\n",
        "    \"사람인\",\n",
        "    \"saramin\",\n",
        "    \"숨고\",\n",
        "    \"soomgo\",\n",
        "    \"스마일게이트AI\",\n",
        "    \"smilegate\",\n",
        "    \"스케터랩\",\n",
        "    \"scatterlab\",\n",
        "    \"스타일쉐어\",\n",
        "    \"styleshare\",\n",
        "    \"쏘카\",\n",
        "    \"socarcorp\",\n",
        "    \"아이디어스\",\n",
        "    \"idus\",\n",
        "    \"야놀자\",\n",
        "    \"yanolja\",\n",
        "    \"야놀자클라우드\",\n",
        "    \"yanoljacloud-tech\",\n",
        "    \"엔라이즈\",\n",
        "    \"nrise\",\n",
        "    \"여기어때\",\n",
        "    \"gccompany\",\n",
        "    \"오일나우\",\n",
        "    \"왓챠\",\n",
        "    \"watcha\",\n",
        "    \"요기요\",\n",
        "    \"yogiyo\",\n",
        "    \"우아한형제들\",\n",
        "    \"woowahan\",\n",
        "    \"원티드\",\n",
        "    \"wantedjobs\",\n",
        "    \"지마켓\",\n",
        "    \"gmarket\",\n",
        "    \"직방\",\n",
        "    \"zigbang\",\n",
        "    \"카카오\",\n",
        "    \"kakao\",\n",
        "    \"카카오엔터프라이즈\",\n",
        "    \"kakaoenterprise\",\n",
        "    \"카카오페이\",\n",
        "    \"kakaopay\",\n",
        "    \"컬리\",\n",
        "    \"kurly\",\n",
        "    \"코인원\",\n",
        "    \"coinone\",\n",
        "    \"쿠팡\",\n",
        "    \"coupang-engineering\",\n",
        "    \"크몽\",\n",
        "    \"kmong\",\n",
        "    \"클라우드메이트\",\n",
        "    \"cloudmt\",\n",
        "    \"테이블링\",\n",
        "    \"tabling\",\n",
        "    \"토스\",\n",
        "    \"toss\",\n",
        "    \"포스타입\",\n",
        "    \"postype\",\n",
        "    \"하이퍼커넥트\",\n",
        "    \"hyperconnect\",\n",
        "    \"헤이딜러\",\n",
        "    \"prnd\",\n",
        "    \"화해\",\n",
        "    \"hwahae\",\n",
        "]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### stopword: from DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from crawling import config\n",
        "# #  from src.crawling.config import connect, close\n",
        "\n",
        "# conn = config.connect()\n",
        "# curs = conn.cursor()\n",
        "\n",
        "# channel_sql = \"\"\"select name from channel group by name\"\"\"\n",
        "# curs.execute(channel_sql)\n",
        "# channel_info = curs.fetchall()\n",
        "\n",
        "# channels = []\n",
        "# for channel in channel_info:\n",
        "#     channels.extend(channel)\n",
        "# print(channels)\n",
        "\n",
        "# config.close()\n",
        "\n",
        "# stopwords.extend(channels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLHwJBTIyx9_"
      },
      "source": [
        "## keyword 추출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "t-uzs8GQyslO"
      },
      "outputs": [],
      "source": [
        "def keyword_extractor(bert:str, documents:List[str]): # -> (List[Tuple[str, float]] | List[List[Tuple[str, float]]]):\n",
        "  model = BertModel.from_pretrained(bert)\n",
        "  model = model.cuda()\n",
        "  kw_model = KeyBERT(model)\n",
        "  keywords = kw_model.extract_keywords(documents, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=20)\n",
        "  return keywords\n",
        "# model_100langs = BertModel.from_pretrained('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')\n",
        "# kw_model_100langs = KeyBERT(model_100langs)\n",
        "# keywords_100langs = kw_model_100langs.extract_keywords(text, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE cuda:0\n"
          ]
        }
      ],
      "source": [
        "# gpu 사용법\n",
        "# https://github.com/MaartenGr/KeyBERT/issues/108\n",
        "# model = SentenceTransformer(\n",
        "#     \"<hf_model_name>\",\n",
        "#     device=\"cuda:0\"\n",
        "# )\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "model = model.cuda()\n",
        "print(\"DEVICE\", model.device)\n",
        "# hf_model = KeyBERT(model)\n",
        "\n",
        "# https://www.kaggle.com/code/accountstatus/shopee-competition-using-bert-model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "CXJC5qTf6j3n"
      },
      "outputs": [],
      "source": [
        "# print(os.getcwd())\n",
        "# base_path = \"/content/drive/MyDrive/쳇, 6pt: 비스킷(biskuit)/dataset/content/\"\n",
        "import csv\n",
        "import datetime\n",
        "\n",
        "base_path = '../../../data/content/'\n",
        "csv_path = '../../data/crawling_data.csv'\n",
        "\n",
        "f = open(csv_path, \"r\", encoding=\"utf-8\")\n",
        "\n",
        "csvReader = csv.reader(f)\n",
        "\n",
        "def extract_keyword(base_path:str):\n",
        "    documents = []\n",
        "\n",
        "    for row in csvReader:\n",
        "        if len(row) == 0:\n",
        "            continue\n",
        "        title = row[1]\n",
        "        created_date = row[3]\n",
        "        channel = row[2]\n",
        "        categories = row[4].split()\n",
        "        print(channel + '_' + title + ' '.join(categories))\n",
        "        # \"Design\", \"Product\", \"Culture\", \"Conference\" 태그만 달려있는 글 분석 제외\n",
        "        if len(categories) != 0 and len(set(categories) - set([\"Design\", \"Product\", \"Culture\", \"Conference\"])) == 0: continue\n",
        "        content_path = base_path + created_date + '/' + channel + '_' + title + '.txt'\n",
        "        file = open(content_path, 'r', encoding='utf-8')\n",
        "        text = file.read()\n",
        "        if len(text.split('\\n')) > 50:\n",
        "            summary = summarize(text)\n",
        "        else:\n",
        "            summary = text\n",
        "        # nouns = noun_extractor(text)\n",
        "        nouns = noun_extractor(summary)\n",
        "        text = ' '.join(nouns)\n",
        "        documents.append(text)\n",
        "    return keyword_extractor('skt/kobert-base-v1', documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "64ezQHBbiJ9K"
      },
      "outputs": [],
      "source": [
        "def extract_keyword2(base_path:str):\n",
        "  content_dir_list = os.scandir(base_path)\n",
        "  documents = []\n",
        "  for content_dir in content_dir_list:\n",
        "      if not content_dir.is_dir(): continue\n",
        "      content_list = os.scandir(content_dir)\n",
        "      for content in content_list:\n",
        "          if not content.is_file(): continue\n",
        "          file = open(content.path, \"r\")\n",
        "          if len(text.split('\\n')) > 50:\n",
        "            summary = summarize(text)\n",
        "          else:\n",
        "            summary = text\n",
        "          # nouns = noun_extractor(text)\n",
        "          nouns = noun_extractor(summary)\n",
        "          text = ' '.join(nouns)\n",
        "          documents.append(text)\n",
        "  # print(text)\n",
        "  keyword_extractor('skt/kobert-base-v1', documents)\n",
        "  # keyword_extractor('paraphrase-multilingual-MiniLM-L12-v2', documents)   # multilingual!\n",
        "  # keyword_extractor('paraphrase-multilingual-mpnet-base-v2', documents)   # 시간 더 오래걸리고 결과 더 조음!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "브랜디_Jest를 이용한 vuejs 프로젝트에 Unit Test 도입\n",
            "네이버클라우드_Ncloud Elasticsearch Service 활용 아파치 로그 분석하기\n",
            "스케터랩_Transformer - Harder  Better  Faster  Stronger\n",
            "네이버클라우드_[NBP 기술&경험] 시대의 흐름  gRPC 깊게 파고들기 #2\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../../../data/content/2020-01-03/네이버클라우드_[NBP 기술&경험] 시대의 흐름  gRPC 깊게 파고들기 #2.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[117], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m faster_keywords \u001b[39m=\u001b[39m extract_keyword(base_path)\n\u001b[0;32m      2\u001b[0m faster_counter \u001b[39m=\u001b[39m Counter(faster_keywords)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(faster_counter\u001b[39m.\u001b[39mmost_common(\u001b[39m100\u001b[39m))\n",
            "Cell \u001b[1;32mIn[114], line 27\u001b[0m, in \u001b[0;36mextract_keyword\u001b[1;34m(base_path)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(categories) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(categories) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mDesign\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mProduct\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCulture\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mConference\u001b[39m\u001b[39m\"\u001b[39m])) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     26\u001b[0m content_path \u001b[39m=\u001b[39m base_path \u001b[39m+\u001b[39m created_date \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m channel \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m title \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 27\u001b[0m file \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(content_path, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     28\u001b[0m text \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mread()\n\u001b[0;32m     29\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(text\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)) \u001b[39m>\u001b[39m \u001b[39m50\u001b[39m:\n",
            "File \u001b[1;32me:\\Users\\Randall\\anaconda3\\envs\\biscuit\\lib\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../data/content/2020-01-03/네이버클라우드_[NBP 기술&경험] 시대의 흐름  gRPC 깊게 파고들기 #2.txt'"
          ]
        }
      ],
      "source": [
        "faster_keywords = extract_keyword(base_path)\n",
        "faster_counter = Counter(faster_keywords)\n",
        "print(faster_counter.most_common(100))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
