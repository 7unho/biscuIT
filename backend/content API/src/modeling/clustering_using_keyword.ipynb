{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%matplotlib inline\n",
    "import sys\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "import csv\n",
    "import re\n",
    "# TextRank\n",
    "from gensim.summarization.summarizer import summarize\n",
    "# kiwi: Tokenizer\n",
    "from kiwipiepy import Kiwi\n",
    "# keyBert\n",
    "from keybert import KeyBERT\n",
    "from transformers import BertModel\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = '/'.join(os.getcwd().split(\"\\\\\")[:-1])\n",
    "sys.path.append(module_path)\n",
    "sys.path.append(module_path + '/crawling')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data 전처리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kiwi = Kiwi()\n",
    "# 명사 추출 함수\n",
    "def noun_extractor(text):\n",
    "    results = []\n",
    "    result = kiwi.analyze(text)\n",
    "    # print(result)\n",
    "    for token, pos, _, _ in result[0][0]:\n",
    "        if len(token) != 1 and pos.startswith('N') or pos.startswith('SL'):\n",
    "            results.append(token)\n",
    "    return results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불용어(stopwords) 제거"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stopwords: list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\n",
    "    # channel\n",
    "    \"11번가\",\n",
    "    \"11st-tech\",\n",
    "    \"29CM\",\n",
    "    \"29cm\",\n",
    "    \"AWS\",\n",
    "    \"amazon\",\n",
    "    \"class101\",\n",
    "    \"GS리테일\",\n",
    "    \"gsretail\",\n",
    "    \"NHN\",\n",
    "    \"nhncloud\",\n",
    "    \"ToastUI\",\n",
    "    \"네이버D2\",\n",
    "    \"d2\",\n",
    "    \"naver\",\n",
    "    \"네이버클라우드\",\n",
    "    \"naver-cloud-platform\",\n",
    "    \"네이버플레이스\",\n",
    "    \"naver-place-dev\",\n",
    "    \"넷마블\",\n",
    "    \"netmarble\",\n",
    "    \"다나와\",\n",
    "    \"danawalab\",\n",
    "    \"당근마켓\",\n",
    "    \"daangn\",\n",
    "    \"데보션\",\n",
    "    \"devocean\",\n",
    "    \"sk\",\n",
    "    \"데브시스터즈\",\n",
    "    \"devsisters\",\n",
    "    \"드라마앤컴퍼니\",\n",
    "    \"dramancompany\",\n",
    "    \"라인\",\n",
    "    \"linecorp\",\n",
    "    \"레모네이드\",\n",
    "    \"lemonade-engineering\",\n",
    "    \"롯데on\",\n",
    "    \"lotteon\",\n",
    "    \"루닛\",\n",
    "    \"lunit\",\n",
    "    \"마이리얼트립\",\n",
    "    \"myrealtrip-product\",\n",
    "    \"메가존클라우드\",\n",
    "    \"ctc-mzc\",\n",
    "    \"무신사\",\n",
    "    \"musinsa\",\n",
    "    \"버즈빌\",\n",
    "    \"buzzvil\",\n",
    "    \"브랜디\",\n",
    "    \"brandi\",\n",
    "    \"사람인\",\n",
    "    \"saramin\",\n",
    "    \"숨고\",\n",
    "    \"soomgo\",\n",
    "    \"스마일게이트AI\",\n",
    "    \"smilegate\",\n",
    "    \"스케터랩\",\n",
    "    \"scatterlab\",\n",
    "    \"스타일쉐어\",\n",
    "    \"styleshare\",\n",
    "    \"쏘카\",\n",
    "    \"socarcorp\",\n",
    "    \"아이디어스\",\n",
    "    \"idus\",\n",
    "    \"야놀자\",\n",
    "    \"yanolja\",\n",
    "    \"야놀자클라우드\",\n",
    "    \"yanoljacloud-tech\",\n",
    "    \"엔라이즈\",\n",
    "    \"nrise\",\n",
    "    \"여기어때\",\n",
    "    \"gccompany\",\n",
    "    \"오일나우\",\n",
    "    \"왓챠\",\n",
    "    \"watcha\",\n",
    "    \"요기요\",\n",
    "    \"yogiyo\",\n",
    "    \"우아한형제들\",\n",
    "    \"woowahan\",\n",
    "    \"원티드\",\n",
    "    \"wantedjobs\",\n",
    "    \"지마켓\",\n",
    "    \"gmarket\",\n",
    "    \"직방\",\n",
    "    \"zigbang\",\n",
    "    \"카카오\",\n",
    "    \"kakao\",\n",
    "    \"카카오엔터프라이즈\",\n",
    "    \"kakaoenterprise\",\n",
    "    \"카카오페이\",\n",
    "    \"kakaopay\",\n",
    "    \"컬리\",\n",
    "    \"kurly\",\n",
    "    \"코인원\",\n",
    "    \"coinone\",\n",
    "    \"쿠팡\",\n",
    "    \"coupang-engineering\",\n",
    "    \"크몽\",\n",
    "    \"kmong\",\n",
    "    \"클라우드메이트\",\n",
    "    \"cloudmt\",\n",
    "    \"테이블링\",\n",
    "    \"tabling\",\n",
    "    \"토스\",\n",
    "    \"toss\",\n",
    "    \"포스타입\",\n",
    "    \"postype\",\n",
    "    \"하이퍼커넥트\",\n",
    "    \"hyperconnect\",\n",
    "    \"헤이딜러\",\n",
    "    \"prnd\",\n",
    "    \"화해\",\n",
    "    \"hwahae\",\n",
    "]\n",
    "\n",
    "stopwords.extend([\n",
    "    '사용', '개발자', '사용자', '기반', '프로젝트', '이용', '코드', '기술', '서비스', '활용', '적용', '개발', '소개', '실행', '안녕하세요', 'line', '진행', '관리', '과정', 'developer', '비즈니스', '프로그래밍', '회사', '소프트웨어', '유저',   '프로그램', 'user',  'service',  '포스팅', 'tech',  '다운로드', '저장소', 'google', '동료',  'project', '방법',  'japanese', 'code', '효율', '도구', '공유', '코딩', '관련', '제품', '이미지', '화면', '얘기', '접근', '광고', '리멤버', '대화', '조직', '링크', '컴퍼니', '오픈', '엔지니어', '문서', 'post', '태그', 'hwang', '예산', '가시', '바탕', 'part', '자료', '버즈빌', '스타트업',\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyword 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_extractor(bert:str, documents:List[str]): # -> (List[Tuple[str, float]] | List[List[Tuple[str, float]]]):\n",
    "    model = BertModel.from_pretrained(bert)\n",
    "    model = model.cuda()\n",
    "    kw_model = KeyBERT(model)\n",
    "    keywords = kw_model.extract_keywords(documents, keyphrase_ngram_range=(1, 1), stop_words=None, top_n=20)\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../../../data/content/'\n",
    "csv_path = '../../data/crawling_data.csv'\n",
    "\n",
    "f = open(csv_path, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "csvReader = csv.reader(f)\n",
    "\n",
    "def extract_keyword(base_path:str):\n",
    "    documents = []\n",
    "\n",
    "    for row in csvReader:\n",
    "        if len(row) == 0:\n",
    "            continue\n",
    "        title = row[1]\n",
    "        created_date = row[3]\n",
    "        channel = row[2]\n",
    "        categories = row[4].split()\n",
    "        print(channel + '_' + title + ' '.join(categories))\n",
    "        # \"Design\", \"Product\", \"Culture\", \"Conference\" 태그만 달려있는 글 분석 제외\n",
    "        if len(categories) != 0 and len(set(categories) - set([\"Design\", \"Product\", \"Culture\", \"Conference\"])) == 0: continue\n",
    "        file_name = re.sub('[\\/:*?\"<>|],', \"\", channel + '_' + title)\n",
    "        content_path = base_path + created_date + '/' + re.sub('&', '_', file_name) + '.txt'\n",
    "        file = open(content_path, 'r', encoding='utf-8')\n",
    "        text = file.read()\n",
    "        if len(text.split('\\n')) > 50:\n",
    "            summary = summarize(text)\n",
    "        else:\n",
    "            summary = text\n",
    "        nouns = noun_extractor(summary)\n",
    "        text = ' '.join(nouns)\n",
    "        documents.append(text)\n",
    "    return keyword_extractor('skt/kobert-base-v1', documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faster_keywords = extract_keyword(base_path)\n",
    "faster_counter = Counter(faster_keywords)\n",
    "print(faster_counter.most_common(100))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
